{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "147e576c",
            "metadata": {},
            "source": [
                "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/edit-state-human-feedback.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239520-lesson-3-editing-state-and-human-feedback)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3b2f2448-21c3-4196-9e61-0b47e7d0048b",
            "metadata": {},
            "source": [
                "# Editing graph state\n",
                "\n",
                "## Review\n",
                "\n",
                "We discussed motivations for human-in-the-loop:\n",
                "\n",
                "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
                "\n",
                "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
                "\n",
                "(3) `Editing` - You can modify the state \n",
                "\n",
                "We showed how breakpoints support user approval, but don't yet know how to modify our graph state once our graph is interrupted!\n",
                "\n",
                "## Goals\n",
                "\n",
                "Now, let's show how to directly edit the graph state and insert human feedback."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "95d26b8c-d958-4d21-9ca4-4636d3dfe45c",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture --no-stderr\n",
                "%pip install --quiet -U langgraph langchain_openai langgraph_sdk langgraph-prebuilt"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "65a8df1f-a76a-4803-a532-ea9802106ac8",
            "metadata": {},
            "source": [
                "## Editing state \n",
                "\n",
                "Previously, we introduced breakpoints.\n",
                "\n",
                "We used them to interrupt the graph and await user approval before executing the next node.\n",
                "\n",
                "But breakpoints are also [opportunities to modify the graph state](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/).\n",
                "\n",
                "Let's set up our agent with a breakpoint before the `assistant` node."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bcf24f05-ac2b-455e-846c-0c50ac86e1f4",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "def multiply(a: int, b: int) -> int:\n",
                "    \"\"\"Multiply a and b.\n",
                "\n",
                "    Args:\n",
                "        a: first int\n",
                "        b: second int\n",
                "    \"\"\"\n",
                "    return a * b\n",
                "\n",
                "# This will be a tool\n",
                "def add(a: int, b: int) -> int:\n",
                "    \"\"\"Adds a and b.\n",
                "\n",
                "    Args:\n",
                "        a: first int\n",
                "        b: second int\n",
                "    \"\"\"\n",
                "    return a + b\n",
                "\n",
                "def divide(a: int, b: int) -> float:\n",
                "    \"\"\"Divide a by b.\n",
                "\n",
                "    Args:\n",
                "        a: first int\n",
                "        b: second int\n",
                "    \"\"\"\n",
                "    return a / b\n",
                "\n",
                "tools = [add, multiply, divide]\n",
                "llm = ChatOpenAI(model=\"qwen3:8b\", base_url=\"http://localhost:11434/v1\", api_key=\"\")\n",
                "llm_with_tools = llm.bind_tools(tools)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5dfe84af-5c62-4c3f-8ed7-96b5261f0b7b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Image, display\n",
                "\n",
                "from langgraph.checkpoint.memory import MemorySaver\n",
                "from langgraph.graph import MessagesState\n",
                "from langgraph.graph import START, StateGraph\n",
                "from langgraph.prebuilt import tools_condition, ToolNode\n",
                "\n",
                "from langchain_core.messages import HumanMessage, SystemMessage\n",
                "\n",
                "# System message\n",
                "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
                "\n",
                "# Node\n",
                "def assistant(state: MessagesState):\n",
                "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
                "\n",
                "# Graph\n",
                "builder = StateGraph(MessagesState)\n",
                "\n",
                "# Define nodes: these do the work\n",
                "builder.add_node(\"assistant\", assistant)\n",
                "builder.add_node(\"tools\", ToolNode(tools))\n",
                "\n",
                "# Define edges: these determine the control flow\n",
                "builder.add_edge(START, \"assistant\")\n",
                "builder.add_conditional_edges(\n",
                "    \"assistant\",\n",
                "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
                "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
                "    tools_condition,\n",
                ")\n",
                "builder.add_edge(\"tools\", \"assistant\")\n",
                "\n",
                "memory = MemorySaver()\n",
                "graph = builder.compile(interrupt_before=[\"assistant\"], checkpointer=memory)\n",
                "\n",
                "# Show\n",
                "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "92a47fd5-1f60-41dc-9206-698ed8ece530",
            "metadata": {},
            "source": [
                "Let's run!\n",
                "\n",
                "We can see the graph is interrupted before the chat model responds. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2ce488d-00e4-492e-a62c-dd98702c313f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Input\n",
                "initial_input = {\"messages\": \"Multiply 2 and 3\"}\n",
                "\n",
                "# Thread\n",
                "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
                "\n",
                "# Run the graph until the first interruption\n",
                "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
                "    event['messages'][-1].pretty_print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4be478ef-bd60-4d32-8a05-5f56c93a8396",
            "metadata": {},
            "outputs": [],
            "source": [
                "state = graph.get_state(thread)\n",
                "state"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "36ef63a1-2ab8-416d-babf-d35054e294f0",
            "metadata": {},
            "source": [
                "Now, we can directly apply a state update.\n",
                "\n",
                "Remember, updates to the `messages` key will use the `add_messages` reducer:\n",
                " \n",
                "* If we want to over-write the existing message, we can supply the message `id`.\n",
                "* If we simply want to append to our list of messages, then we can pass a message without an `id` specified, as shown below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9179cff1-e529-473a-9ce2-e23b932c2063",
            "metadata": {},
            "outputs": [],
            "source": [
                "graph.update_state(\n",
                "    thread,\n",
                "    {\"messages\": [HumanMessage(content=\"No, actually multiply 3 and 3!\")]},\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d77b8d6a-8c7b-4f7a-b723-121af25ac829",
            "metadata": {},
            "source": [
                "Let's have a look.\n",
                "\n",
                "We called `update_state` with a new message. \n",
                "\n",
                "The `add_messages` reducer appends it to our state key, `messages`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "141b6aab-ec6d-44f3-beb1-6c22ac5f2158",
            "metadata": {},
            "outputs": [],
            "source": [
                "new_state = graph.get_state(thread).values\n",
                "for m in new_state['messages']:\n",
                "    m.pretty_print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e4041959-cc3a-4168-8cf7-06d1711921d8",
            "metadata": {},
            "source": [
                "Now, let's proceed with our agent, simply by passing `None` and allowing it proceed from the current state.\n",
                "\n",
                "We emit the current and then proceed to execute the remaining nodes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f166bed2-87c9-41ec-b235-0305721c2d6b",
            "metadata": {},
            "outputs": [],
            "source": [
                "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
                "    event['messages'][-1].pretty_print()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b18dc1ca",
            "metadata": {},
            "source": [
                "Now, we're back at the `assistant`, which has our `breakpoint`.\n",
                "\n",
                "We can again pass `None` to proceed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f5952731-0170-4589-a399-ee787df35400",
            "metadata": {},
            "outputs": [],
            "source": [
                "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
                "    event['messages'][-1].pretty_print()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "bc22c3e9-b00c-4ead-b752-a682b45b3718",
            "metadata": {},
            "source": [
                "### Editing graph state in Studio\n",
                "\n",
                "**âš ï¸ DISCLAIMER**\n",
                "\n",
                "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
                "\n",
                "```\n",
                "langgraph dev\n",
                "```\n",
                "\n",
                "You should see the following output:\n",
                "```\n",
                "- ðŸš€ API: http://127.0.0.1:2024\n",
                "- ðŸŽ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
                "- ðŸ“š API Docs: http://127.0.0.1:2024/docs\n",
                "```\n",
                "\n",
                "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
                "\n",
                "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "020efeba-fa80-4839-81f9-9ce228f9844e",
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'google.colab' in str(get_ipython()):\n",
                "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "642aabab-f822-4917-9d66-3314ac5008fd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# This is the URL of the local development server\n",
                "from langgraph_sdk import get_client\n",
                "client = get_client(url=\"http://127.0.0.1:2024\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "be74cb09",
            "metadata": {},
            "source": [
                "Our agent is defined in `studio/agent.py`. \n",
                "\n",
                "If you look at the code, you'll see that it *does not* have a breakpoint! \n",
                " \n",
                "Of course, we can add it to `agent.py`, but one very nice feature of the API is that we can pass in a breakpoint!\n",
                "\n",
                "Here, we pass a `interrupt_before=[\"assistant\"]`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c352f9e-6a0f-4a94-a083-b85b0233efa9",
            "metadata": {},
            "outputs": [],
            "source": [
                "initial_input = {\"messages\": \"Multiply 2 and 3\"}\n",
                "thread = await client.threads.create()\n",
                "async for chunk in client.runs.stream(\n",
                "    thread[\"thread_id\"],\n",
                "    \"agent\",\n",
                "    input=initial_input,\n",
                "    stream_mode=\"values\",\n",
                "    interrupt_before=[\"assistant\"],\n",
                "):\n",
                "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
                "    messages = chunk.data.get('messages', [])\n",
                "    if messages:\n",
                "        print(messages[-1])\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "13065dd9-5f43-47d6-ac2a-9dc15c0c54e6",
            "metadata": {},
            "source": [
                "We can get the current state"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4da2c464-3e71-496a-badc-671aeee168b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "current_state = await client.threads.get_state(thread['thread_id'])\n",
                "current_state"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4527bbf1-0927-41a6-aeef-d15e32bbbdc3",
            "metadata": {},
            "source": [
                "We can look at the last message in state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "801ae2d9-0551-46b8-aee2-82293cee4011",
            "metadata": {},
            "outputs": [],
            "source": [
                "last_message = current_state['values']['messages'][-1]\n",
                "last_message"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f0581ba8-db3d-474d-9042-b1c7f3461caf",
            "metadata": {},
            "source": [
                "We can edit it!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86b12be7-7e4a-40d0-8521-dced7c393c71",
            "metadata": {},
            "outputs": [],
            "source": [
                "last_message['content'] = \"No, actually multiply 3 and 3!\"\n",
                "last_message"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f84f2c24-f281-4591-90e5-de3a5547c9da",
            "metadata": {},
            "outputs": [],
            "source": [
                "last_message"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ce7b4280-6ae7-4246-9c87-44e0daa6c654",
            "metadata": {},
            "source": [
                "Remember, as we said before, updates to the `messages` key will use the same `add_messages` reducer. \n",
                "\n",
                "If we want to over-write the existing message, then we can supply the message `id`.\n",
                "\n",
                "Here, we did that. We only modified the message `content`, as shown above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "84d33b6e-32ff-4eca-8114-345e508f3481",
            "metadata": {},
            "outputs": [],
            "source": [
                "await client.threads.update_state(thread['thread_id'], {\"messages\": last_message})"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1f07f0d1-7083-4827-babd-d3702eb59a37",
            "metadata": {},
            "source": [
                "Now, we resume by passing `None`. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ef18d12d-e0a6-487a-9f32-ad30e2634a20",
            "metadata": {},
            "outputs": [],
            "source": [
                "async for chunk in client.runs.stream(\n",
                "    thread[\"thread_id\"],\n",
                "    assistant_id=\"agent\",\n",
                "    input=None,\n",
                "    stream_mode=\"values\",\n",
                "    interrupt_before=[\"assistant\"],\n",
                "):\n",
                "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
                "    messages = chunk.data.get('messages', [])\n",
                "    if messages:\n",
                "        print(messages[-1])\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6a82dd35-cbc8-486d-8e20-10d0c4d138d6",
            "metadata": {},
            "source": [
                "We get the result of the tool call as `9`, as expected."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1d1bb3c7-dc26-4c32-b3df-865f41ef3c73",
            "metadata": {},
            "outputs": [],
            "source": [
                "async for chunk in client.runs.stream(\n",
                "    thread[\"thread_id\"],\n",
                "    assistant_id=\"agent\",\n",
                "    input=None,\n",
                "    stream_mode=\"values\",\n",
                "    interrupt_before=[\"assistant\"],\n",
                "):\n",
                "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
                "    messages = chunk.data.get('messages', [])\n",
                "    if messages:\n",
                "        print(messages[-1])\n",
                "    print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6914c5ca-27e4-421c-835a-9e4327dac12f",
            "metadata": {},
            "source": [
                "## Awaiting user input\n",
                "\n",
                "So, it's clear that we can edit our agent state after a breakpoint.\n",
                "\n",
                "Now, what if we want to allow for human feedback to perform this state update?\n",
                "\n",
                "We'll add a node that [serves as a placeholder for human feedback](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/#setup) within our agent.\n",
                "\n",
                "This `human_feedback` node allow the user to add feedback directly to state.\n",
                " \n",
                "We specify the breakpoint using `interrupt_before` our `human_feedback` node.\n",
                "\n",
                "We set up a checkpointer to save the state of the graph up until this node."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4b475ff-681f-4660-80dd-d6ade7bd48e3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# System message\n",
                "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
                "\n",
                "# no-op node that should be interrupted on\n",
                "def human_feedback(state: MessagesState):\n",
                "    pass\n",
                "\n",
                "# Assistant node\n",
                "def assistant(state: MessagesState):\n",
                "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
                "\n",
                "# Graph\n",
                "builder = StateGraph(MessagesState)\n",
                "\n",
                "# Define nodes: these do the work\n",
                "builder.add_node(\"assistant\", assistant)\n",
                "builder.add_node(\"tools\", ToolNode(tools))\n",
                "builder.add_node(\"human_feedback\", human_feedback)\n",
                "\n",
                "# Define edges: these determine the control flow\n",
                "builder.add_edge(START, \"human_feedback\")\n",
                "builder.add_edge(\"human_feedback\", \"assistant\")\n",
                "builder.add_conditional_edges(\n",
                "    \"assistant\",\n",
                "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
                "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
                "    tools_condition,\n",
                ")\n",
                "builder.add_edge(\"tools\", \"human_feedback\")\n",
                "\n",
                "memory = MemorySaver()\n",
                "graph = builder.compile(interrupt_before=[\"human_feedback\"], checkpointer=memory)\n",
                "display(Image(graph.get_graph().draw_mermaid_png()))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32d4ceb6-a224-4307-8196-3f53d367df5c",
            "metadata": {},
            "source": [
                "We will get feedback from the user.\n",
                "\n",
                "We use `.update_state` to update the state of the graph with the human response we get, as before.\n",
                "\n",
                "We use the `as_node=\"human_feedback\"` parameter to apply this state update as the specified node, `human_feedback`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3fc7bcd6-660c-4a8a-ad8d-e6698dcf6201",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Input\n",
                "initial_input = {\"messages\": \"Multiply 2 and 3\"}\n",
                "\n",
                "# Thread\n",
                "thread = {\"configurable\": {\"thread_id\": \"5\"}}\n",
                "\n",
                "# Run the graph until the first interruption\n",
                "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
                "    event[\"messages\"][-1].pretty_print()\n",
                "    \n",
                "# Get user input\n",
                "user_input = input(\"Tell me how you want to update the state: \")\n",
                "\n",
                "# We now update the state as if we are the human_feedback node\n",
                "graph.update_state(thread, {\"messages\": user_input}, as_node=\"human_feedback\")\n",
                "\n",
                "# Continue the graph execution\n",
                "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
                "    event[\"messages\"][-1].pretty_print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "abf4cf5f-c0cb-4fdb-be6b-271ae4e967e2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Continue the graph execution\n",
                "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
                "    event[\"messages\"][-1].pretty_print()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
